{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Diogo Meneses Franco\n",
        "# RA:2202455 "
      ],
      "metadata": {
        "id": "jjb8zXBFfMB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdcpsL-HuBxX"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "sudo pip install spark\n",
        "sudo pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Importando o spark e o pyspark\n",
        "import spark,pyspark\n",
        "\n",
        "#Importando as bibliotecas do pyspark.sql \n",
        "from pyspark.sql import *\n",
        "\n",
        "#Importando as funções sql do spark\n",
        "#documentação https://spark.apache.org/docs/latest/api/sql/index.html\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "#Importando os tipos de dados do spark\n",
        "#documentação https://spark.apache.org/docs/latest/sql-ref-datatypes.html\n",
        "from pyspark.sql import types as t \n",
        "\n",
        "\n",
        "#Biblioteca datetime\n",
        "from datetime import datetime, date"
      ],
      "metadata": {
        "id": "z-x26EjeuXOg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Criando uma Sessão do Spark (Spark Session)\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"AC5 DataEng\").getOrCreate()"
      ],
      "metadata": {
        "id": "KgAMjvrkubGU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Criar um dataframe lendo o arquivo vendas.parquet\n",
        "\n",
        "df = spark.read.format(\"parquet\").load(\"/content/vendas.parquet\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "ubhyngJruuy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converter a coluna data para o tipo DateType.\n",
        "df = df.withColumn(\"data\",df.data.cast(t.DateType()))\n",
        "df.show()"
      ],
      "metadata": {
        "id": "C-0H-_FQvn75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Criar as colunas, utilizando as funções spark.sql.functions.year, spark.sql.functions.month , spark.sql.functions.dayofMonth:\n",
        "df = df.withColumn(\"ano\",f.year(\"data\"))\n",
        "df = df.withColumn(\"Mes\",f.month(\"data\"))\n",
        "df = df.withColumn(\"Dia\",f.dayofmonth(\"data\"))\n",
        "df.show()"
      ],
      "metadata": {
        "id": "SKfKwjQex68E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Na coluna sms, se caso o valor for null , troque por False (use a função spark.sql.functions.when (df.sms.isNull(),False).otherwise(df.sms)) \n",
        "df.select(df.sms,f.when(df.sms.isNull(),False).otherwise(df.sms)).show()"
      ],
      "metadata": {
        "id": "wXCzKTyVzu1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crie uma coluna chamada “total” que deve ser a multiplicação das colunas “vlr” e “qtd”.\n",
        "df = df.withColumn(\"total\",df.vlr * df.qtd)\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "S3eQ9_T94F7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Grave o dataframe particionado por ano, mes e dia no formato “parquet” de nome “vendas_tratadas”.\n",
        "df.write.format(\"parquet\").mode(\"overwrite\").partitionBy(\"ano\",\"mes\",\"dia\").save(\"vendas_tratadas\")\n"
      ],
      "metadata": {
        "id": "CDWPq3ne4gg7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crie uma tabela temporaria chamada “vendas_table” (df.registerTempTable(“vendas_table”))\n",
        "df.registerTempTable(\"vendas_table\")\n"
      ],
      "metadata": {
        "id": "y_9mUy0h5tC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crie um dataframe spark.sql utilizando a query spark.sql(“select distinct cidade from vendas_table”). Grave esse dataframe no formato “parquet” de nome  “cidade”\n",
        "df_cid = spark.sql(\"select distinct cidade from vendas_table\")\n",
        "df_cid.write.format(\"parquet\").mode(\"overwrite\").save(\"cidade\")\n",
        "df_cid.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "uWAzjit36EfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crie um novo dataframe chamado “produto” selecionando as colunas: “des_produto”,”des_familia”, “des_secao”, “des_categoria”,”des_sub_categoria”. Remova possíveis espaços no começo e no final das colunas (spark.sql.functions.trim). Deixe apenas valores únicos (df.distinct()). Grave esse dataframe como parquet “produto”.\n",
        "\n",
        "df_prod = spark.sql(\"select distinct des_produto, des_familia, des_secao, des_categoria, des_sub_categoria from vendas_table\")\n",
        "df_prod.show()"
      ],
      "metadata": {
        "id": "-O8mdMkB61pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for colunas in df_prod.columns:\n",
        "    df_prod = df_prod.withColumn(colunas, f.trim(f.col(colunas)))\n",
        "df_prod.show()"
      ],
      "metadata": {
        "id": "z3yvL-EdEK5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format(\"parquet\").mode(\"overwrite\").save(\"produto\")\n"
      ],
      "metadata": {
        "id": "S0OSqZtdEABK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crie um dataframe chamado “venda_mulheres”, selecionando todas as colunas e filtrando apenas vendas realizadas pelo sexo “F”, grave esse dataframe no formato json “venda_mulher”. r\n",
        "df_f = spark.sql(\"select * from vendas_table\")\n",
        "df_f.show()\n"
      ],
      "metadata": {
        "id": "dqB9TKIpUt0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_f = df_f.filter(df_f.sexo=='F')\n",
        "df_f.show()"
      ],
      "metadata": {
        "id": "SsUOIjI8XIJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_f.write.format(\"json\").mode(\"overwrite\").save(\"venda_mulher\")"
      ],
      "metadata": {
        "id": "3xjyDlsrYNkK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crie um dataframe chamado “venda_unicas”, selecionando todas as colunas e filtrando apenas vendas com a qtd igual 1 e o vlr menor que 10. Grave esse dataframe no formato csv “venda_unica”. (df.write.format(“csv”).save(“venda_unica”)\n",
        "\n",
        "df_vu = spark.sql(\"select * from vendas_table\")\n",
        "df_vu = df_vu.filter(\"qtd==1 and vlr <10\")\n",
        "df_vu.show()\n"
      ],
      "metadata": {
        "id": "4BLdy6ADY3lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_vu.write.format(\"csv\").mode(\"overwrite\").save(\"venda_unica\")"
      ],
      "metadata": {
        "id": "4-oZn5i9e15r"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}